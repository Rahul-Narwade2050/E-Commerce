import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from pyspark.sql import*
from pyspark.sql.types import*
from pyspark.sql.functions import*
from pyspark.sql import SparkSession
spark= SparkSession.builder.getOrCreate()

args=getResolvedOptions(sys.argv,['buck','file']
bucket_name=args['buck']
file_name=args['file']

input_file_path='s3://{}/{}'.format(bucket_name,file_name)
cust=spark.read.format('csv').option('header',True).option('sep',',').option('inferSchema',True).load(input_file_path)
#check cust schemas 
cust.printSchema()

#counting number of rows
cust.count()

#counting unique records
cust.distinct().count()

#it will give statistical result count,max,min,std values
cust.describe().display()

#it will droping duplicates values if available
cust=cust.dropDuplicates()

#na.fill used to filling  any values in null records
cust=cust.na.fill()

#here we replacing country id using when other functions
cust=cust.withColumn('country_id',when(col('country_id')==52771, 52781).when(col('country_id')==52781, 52771).otherwise(col('country_id')))

#joining two table cust and city to get country name
cust_city_join=cust.join(city,cust.country_id==city.COUNTRY_ID,'inner')

#calculating  country and statewise customer
customer_city_state=cust_city_join.groupBy('COUNTRY_NAME','CUST_STATE_PROVINCE').agg(count('CUST_STATE_PROVINCE').alias('cust_no')).orderBy(col('cust_no').desc())

#moving data to redshift warehouse
customer_city_state.write \
    .format("io.github.spark_redshift_community.spark.redshift") \
    .option("url", url) \
    .option("dbtable", "customer_city_state") \
    .option("tempdir", "s3://path/for/temp/data") \
    .option("aws_iam_role", "aws_iam_role_arn") \
    .save()
job.commit()
